{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/moseskim/bert_nlp/blob/main/section_4/01_simple_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"uv454lBK1YCc"},"source":["# 간단한 파인 튜닝\n","\n","최소한의 코드로 파인 튜닝을 구현합니다.  \n","사전 학습 완료 모델을 몇 차례 추가로 훈련합니다."]},{"cell_type":"markdown","metadata":{"id":"L_Ozfz3NhltP"},"source":["## 라이브러리 설치\n","\n","라이브러리인 Transformers를 설치합니다.  \n","https://huggingface.co/transformers/index.html"]},{"cell_type":"code","metadata":{"id":"Y_mDYVlb-sqi"},"source":["!pip install transformers"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5nP8RaW-SLD-"},"source":["## 모델 로딩\n","\n","`BertForSequenceClassification`을 사용해 사전 학습 완료 모델을 로딩합니다."]},{"cell_type":"code","metadata":{"id":"36bQQblhwpqT"},"source":["from transformers import BertForSequenceClassification\n","\n","sc_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", return_dict=True)\n","print(sc_model.state_dict().keys())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jQV44nAdV82l"},"source":["## 최적화 알고리즘\n","\n","여기에서는 최적화 알고리즘으로 `AdamW`를 채용합니다.  \n","`AdamW`는 오리지널 `Adam`의 가중치 감쇠에 관한 식을 변경한 것입니다.  \n","https://arxiv.org/abs/1711.05101"]},{"cell_type":"code","metadata":{"id":"TVoPZhUPw4o7"},"source":["from transformers import AdamW\n","\n","optimizer = AdamW(sc_model.parameters(), lr=1e-5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_Z2KV0V8ajTz"},"source":["## Tokenizer 설정\n","\n","`BertTokenizer`를 사용해 문장을 단어로 분할하고 `id`로 변환합니다.  \n","`BertForSequenceClassification` 모델 훈련 시에는 입력 외에 어텐션 mask를 전달해야 합니다. BertTokenizer`를 사용해 이것을 얻을 수 있습니다."]},{"cell_type":"code","metadata":{"id":"3bSSCHNExo-I"},"source":["from transformers import BertTokenizer\n","\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","sentences = [\"I love baseball.\", \"I hate baseball.\"]\n","tokenized = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True)\n","print(tokenized)\n","\n","x = tokenized[\"input_ids\"]\n","attention_mask = tokenized[\"attention_mask\"]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BMhbOWRXXT0n"},"source":["## 파인 튜닝\n","\n","사전에 학습 완료한 모델을 추가로 훈련시킵니다."]},{"cell_type":"code","metadata":{"id":"b1MqceDP1IeR"},"source":["import torch\n","from torch.nn import functional as F\n","import matplotlib.pyplot as plt\n","\n","sc_model.train()\n","t = torch.tensor([1,0])  # 문장 분류\n","\n","weight_record = []  # 가중치 기록\n","\n","for i in range(100):\n","    y = sc_model(x, attention_mask=attention_mask)\n","    loss = F.cross_entropy(y.logits, t)\n","    loss.backward()\n","    optimizer.step()\n","\n","    weight = sc_model.state_dict()[\"bert.encoder.layer.11.output.dense.weight\"][0][0].item()\n","    weight_record.append(weight)\n","\n","plt.plot(range(len(weight_record)), weight_record)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mWcBnNXOYy32"},"source":["추가 훈련에 따라 가중치가 조정된 상태를 확인할 수 있습니다."]},{"cell_type":"code","source":[],"metadata":{"id":"090RBb_EVp1o"},"execution_count":null,"outputs":[]}]}