{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/moseskim/bert_nlp/blob/main/section_5/01_news_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"vrgegdDZjf8E"},"source":["# 일본어 문장 분류\n","\n","일본어 데이터셋으로 BERT 모델을 파인 튜닝하고, 뉴스를 분류합니다."]},{"cell_type":"markdown","metadata":{"id":"m6moZnLFkFwr"},"source":["## 라이브러리 설치\n","\n","라이브러리인 Transformers, 및 `nlp`를 설치합니다."]},{"cell_type":"code","metadata":{"id":"7qg6t5nnBjqs"},"source":["!pip install transformers\n","!pip install nlp\n","!pip install datasets\n","!pip install fugashi\n","!pip install ipadic\n","!pip install sentencepiece\n","!pip install accelerate"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KcHOX9LyZc2g"},"source":["## Google 드라이브와의 연동\n","\n","다음 코드를 실행하고 인증 코드를 사용해 Google 드라이브를 마운트합니다."]},{"cell_type":"code","metadata":{"id":"7h7BA67Ed5wT"},"source":["from google.colab import drive\n","drive.mount(\"/content/drive/\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zliYGLC5g0h2"},"source":["## 데이터셋 로딩\n","\n","Google 드라이브에 저장되어 있는 뉴스 데이터셋을 로딩합니다."]},{"cell_type":"code","metadata":{"id":"jPV3qCYs9STS"},"source":["import glob  # 파일 취득에 사용\n","import os\n","\n","path = \"/content/drive/My Drive/bert_nlp/section_5/text/\"  # 풀더 위치 지정\n","\n","dir_files = os.listdir(path=path)\n","dirs = [f for f in dir_files if os.path.isdir(os.path.join(path, f))]  # 디렉터리 목록\n","\n","text_label_data = []  # 문장과 라벨 셋\n","dir_count = 0  # 디렉터리 수 카운트\n","file_count= 0  # 파일 수 카운트\n","\n","for i in range(len(dirs)):\n","    dir = dirs[i]\n","    files = glob.glob(path + dir + \"/*.txt\")  # 파일 목록\n","    dir_count += 1\n","\n","    for file in files:\n","        if os.path.basename(file) == \"LICENSE.txt\":\n","            continue\n","\n","        with open(file, \"r\") as f:\n","            text = f.readlines()[3:]\n","            text = \"\".join(text)\n","            text = text.translate(str.maketrans({\"\\n\":\"\", \"\\t\":\"\", \"\\r\":\"\", \"\\u3000\":\"\"}))\n","            text_label_data.append([text, i])\n","\n","        file_count += 1\n","        print(\"\\rfiles: \" + str(file_count) + \"dirs: \" + str(dir_count), end=\"\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LADy70wOgyXg"},"source":["## 데이터 저장\n","\n","데이터를 훈련 데이터와 테스트 데이트로 분할하고, `csv` 파일로 Google Drive에 저장합니다."]},{"cell_type":"code","metadata":{"id":"fIyvN2MT4Unl"},"source":["import csv\n","from sklearn.model_selection import train_test_split\n","\n","news_train, news_test =  train_test_split(text_label_data, shuffle=True)  # 훈련용과 테스트용으로 분할\n","news_path = \"/content/drive/My Drive/bert_nlp/section_5/\"\n","\n","with open(news_path+\"news_train.csv\", \"w\") as f:\n","    writer = csv.writer(f)\n","    writer.writerows(news_train)\n","\n","with open(news_path+\"news_test.csv\", \"w\") as f:\n","    writer = csv.writer(f)\n","    writer.writerows(news_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZsgQNMJxpBnW"},"source":["## 모델과 Tokenizer 로딩\n","\n","일본어 사전 학습 완료 모델 및 이와 관련된 Tokenizer를 로딩합니다."]},{"cell_type":"code","metadata":{"id":"9R0HK29fHrf3"},"source":["from transformers import BertForSequenceClassification, BertJapaneseTokenizer\n","\n","sc_model = BertForSequenceClassification.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\", num_labels=9)\n","sc_model.cuda()\n","tokenizer = BertJapaneseTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aWCmm2TjqToE"},"source":["## 데이터셋 로딩\n","\n","저장된 뉴스 데이터를 로딩합니다.  "]},{"cell_type":"code","metadata":{"id":"rfEnNpv9HuXI"},"source":["from datasets import load_dataset\n","\n","def tokenize(batch):\n","    return tokenizer(batch[\"text\"], padding=True, truncation=True, max_length=128)\n","\n","news_path = \"/content/drive/My Drive/bert_nlp/section_5/\"\n","\n","train_data = load_dataset(\"csv\", data_files=news_path+\"news_train.csv\", column_names=[\"text\", \"label\"], split=\"train\")\n","train_data = train_data.map(tokenize, batched=True, batch_size=len(train_data))\n","train_data.set_format(\"torch\", columns=[\"input_ids\", \"label\"])\n","\n","test_data = load_dataset(\"csv\", data_files=news_path+\"news_test.csv\", column_names=[\"text\", \"label\"], split=\"train\")\n","test_data = test_data.map(tokenize, batched=True, batch_size=len(test_data))\n","test_data.set_format(\"torch\", columns=[\"input_ids\", \"label\"])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Y6Fcqmy2rG2"},"source":["## 평가용 함수\n","\n","`sklearn.metrics`를 사용해 모델을 평가하기 위한 함수를 정의합니다."]},{"cell_type":"code","metadata":{"id":"plAZjdkG0FdV"},"source":["from sklearn.metrics import accuracy_score\n","\n","def compute_metrics(result):\n","    labels = result.label_ids\n","    preds = result.predictions.argmax(-1)\n","    acc = accuracy_score(labels, preds)\n","    return {\n","        \"accuracy\": acc,\n","    }"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MjLqAVy7z0T3"},"source":["## Trainer 설정\n","\n","`Trainer` 클래스 및 `TrainingArguments` 클래스를 사용해 훈련을 수행할 Trainer를 설정합니다.  \n","https://huggingface.co/transformers/main_classes/trainer.html   \n","https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments  "]},{"cell_type":"code","metadata":{"id":"ZhaexaAOI3kV"},"source":["from transformers import Trainer, TrainingArguments\n","\n","training_args = TrainingArguments(\n","    output_dir = \"./results\",\n","    num_train_epochs = 2,\n","    per_device_train_batch_size = 8,\n","    per_device_eval_batch_size = 32,\n","    warmup_steps = 500,  # 학습 계수가 0부터 이 단계 수로 상승\n","    weight_decay = 0.01,  # 가중치 감쇠율\n","    logging_dir = \"./logs\",\n","    evaluation_strategy = \"steps\"\n",")\n","\n","trainer = Trainer(\n","    model = sc_model,\n","    args = training_args,\n","    compute_metrics = compute_metrics,\n","    train_dataset = train_data,\n","    eval_dataset = test_data,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o0F5nXKpSCnS"},"source":["## 모델 훈련\n","\n","설정에 기반해 파인 튜닝을 수행합니다."]},{"cell_type":"code","metadata":{"id":"29fkN4UcI4jm"},"source":["trainer.train()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c76zhkQVS2xZ"},"source":["## 모델 평가\n","\n","Trainer의 `evaluate()` 메서드를 사용해 모델을 평가합니다."]},{"cell_type":"code","metadata":{"id":"wIgke21zI6l_"},"source":["trainer.evaluate()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6EFwqzLRUhaB"},"source":["## TensorBoard를 사용한 결과 표시\n","\n","TensorBoard를 사용해 `logs` 폴더에 저장된 학슴 과정을 표시합니다."]},{"cell_type":"code","metadata":{"id":"1vv39tuDJq5n"},"source":["%load_ext tensorboard\n","%tensorboard --logdir logs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a-BscHjHxs0H"},"source":["## 모델 저장\n","\n","훈련 완료 모델을 저장합니다."]},{"cell_type":"code","metadata":{"id":"UvwVcXuIyH7V"},"source":["news_path = \"/content/drive/My Drive/bert_nlp/section_5/\"\n","\n","sc_model.save_pretrained(news_path)\n","tokenizer.save_pretrained(news_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zZuJCZBK0RJx"},"source":["## 모델 로딩\n","\n","저장 완료 모델을 로딩합니다.  "]},{"cell_type":"code","metadata":{"id":"ZWtcQRuP0X45"},"source":["loaded_model = BertForSequenceClassification.from_pretrained(news_path)\n","loaded_model.cuda()\n","loaded_tokenizer = BertJapaneseTokenizer.from_pretrained(news_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rq2zZ99R3Hs7"},"source":["## 일본어 뉴스 분류\n","\n","로딩한 모델을 사용해 뉴스를 분류합니다."]},{"cell_type":"code","metadata":{"id":"dFOIjY511WVK"},"source":["import glob  # ファイルの取得に使用\n","import os\n","import torch\n","\n","category = \"movie-enter\"\n","sample_path = \"/content/drive/My Drive/bert_nlp/section_5/text/\"  # 폴더 위치 지정\n","files = glob.glob(sample_path + category + \"/*.txt\")  # 파일 목록\n","file = files[12]  # 임의의 뉴스\n","\n","dir_files = os.listdir(path=sample_path)\n","dirs = [f for f in dir_files if os.path.isdir(os.path.join(sample_path, f))]  # 디렉터리 목록\n","\n","with open(file, \"r\") as f:\n","    sample_text = f.readlines()[3:]\n","    sample_text = \"\".join(sample_text)\n","    sample_text = sample_text.translate(str.maketrans({\"\\n\":\"\", \"\\t\":\"\", \"\\r\":\"\", \"\\u3000\":\"\"}))\n","\n","print(sample_text)\n","\n","max_length = 512\n","words = loaded_tokenizer.tokenize(sample_text)\n","word_ids = loaded_tokenizer.convert_tokens_to_ids(words)  # 단어를 인덱스로 변환\n","word_tensor = torch.tensor([word_ids[:max_length]])  # 텐서로 변환\n","\n","x = word_tensor.cuda()  # GPU 대응\n","y = loaded_model(x)  # 예측\n","pred = y[0].argmax(-1)  # 최댓값의 인덱스\n","print(\"result:\", dirs[pred])"],"execution_count":null,"outputs":[]}]}