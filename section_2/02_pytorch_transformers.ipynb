{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/moseskim/bert_nlp/blob/main/section_2/02_pytorch_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"KcHOX9LyZc2g"},"source":["# PyTorch-Transformers\n","\n","BERT 구현을 위해 자연 언어 처리 라이브러리인 PyTorch-Transformers를 학습합니다.  \n","PyTorch-Transformers는 다음 기본 클래스를 중심으로 구성됩니다.  \n","* `BertModel`\n","* `BertConfig`\n","* `BertTokenizer`"]},{"cell_type":"markdown","metadata":{"id":"L_Ozfz3NhltP"},"source":["## 라이브러리 설치\n","\n","PyTorch-Transformers와 필요한 라이브러리를 설치합니다."]},{"cell_type":"code","metadata":{"id":"Y_mDYVlb-sqi"},"source":["!pip install folium==0.2.1\n","!pip install urllib3==1.25.11\n","!pip install pytorch-transformers==1.2.0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nMfvi2uVkItT"},"source":["## PyTorch-Transformers 모델\n","\n","PyTorch-Transformers에서는 다양한 훈련 완료 모델을 다루는 클래스를 제공합니다.  \n","다음 코드에서는 문장의 일부를 Mask하는 문제, `BertForMaskedLM` 모델을 설정합니다.  \n","https://huggingface.co/transformers/model_doc/bert.html#bertformaskedlm  \n","\n","`BertForMaskedLM`은 베이스가 되는 모델인 `PreTrainedModel`을 상속합니다.  \n","https://huggingface.co/transformers/main_classes/model.html#transformers.PreTrainedModel\n","\n","그리고 `BertForMaskedLM`은 `nn.Module` 클래스를 상속하고 있으므로, 일반적인 PyTorch의 모델로 사용할 수 있습니다."]},{"cell_type":"code","metadata":{"id":"E9Hv5L2HGDmI"},"source":["import torch\n","from pytorch_transformers import BertForMaskedLM\n","\n","msk_model = BertForMaskedLM.from_pretrained('bert-base-uncased')  # 훈련 완료 매개변수\n","print(msk_model)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n3V7quRwp1jn"},"source":["최종적으로 단어 수인 `30522` 클래스로 분류하는 문제임을 알 수 있습니다.\n","\n","마찬가지로 문장을 분류하는 문제, `BertForSequenceClassification` 모델을 설정합니다.  \n","https://huggingface.co/transformers/model_doc/bert.**html**#bertforsequenceclassification"]},{"cell_type":"code","metadata":{"id":"7VBwLQZBJyEh"},"source":["from pytorch_transformers import BertForSequenceClassification\n","\n","sc_model = BertForSequenceClassification.from_pretrained('bert-base-uncased')  # 훈련 완료 매개변수 로딩\n","print(sc_model)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c8k0H8AgshmY"},"source":["`out_features=2`이므로 문장을 2개 클래스로 분류하는 문제임을 알 수 있습니다."]},{"cell_type":"markdown","metadata":{"id":"rMlz4gJQucPB"},"source":["# BERT 설정\n","\n","`BertConfig` 클래스를 사용해 모델 설정을 수행할 수 있습니다."]},{"cell_type":"code","metadata":{"id":"F54bOxW6uGBX"},"source":["from pytorch_transformers import BertConfig\n","\n","config = BertConfig.from_pretrained(\"bert-base-uncased\")\n","print(config)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nMQKVuZyv-sC"},"source":["## Tokenizer\n","\n","`BertTokenizer` 클래스를 사용해 훈련 완료 데이터에 기반해 형태소 분석을 수행할 수 있습니다."]},{"cell_type":"code","metadata":{"id":"zspnwnNHxIEz"},"source":["from pytorch_transformers import BertTokenizer\n","\n","text = \"I have a pen. I have an apple.\"\n","\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","words = tokenizer.tokenize(text)\n","print(words)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"OVINTw9tYWFc"},"execution_count":null,"outputs":[]}]}